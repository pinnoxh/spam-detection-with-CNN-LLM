{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c7093d4-1be6-41a2-92b0-0e186794a66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 12:06:11.664602: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-12 12:06:11.682581: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-12 12:06:11.688078: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-12 12:06:11.702282: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-12 12:06:12.719999: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/basilmusyaffa19/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import pickle\n",
    "nltk.download('punkt')\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f258178-edcd-4a59-8d1e-7ed72b22e1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "1.13.0+cu117\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5aebe2d-8177-4f46-8b2d-6c467171341c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 11.7\n",
      "Number of GPUs: 1\n",
      "GPU 0: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs: {num_gpus}\")\n",
    "\n",
    "if num_gpus > 0:\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2271fd6f-a8f5-4a72-8e0e-d7c5562b1b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "476f8748-5e1f-40c1-8fa6-5936a6262b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>teks</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>promo beli paket flash mulai aplikasi my telko...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gb hari rp ribu spesial buat kamu pilih aktif ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plg hormat sisa kuota flash download aplikasi ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mohon hormat sisa kuota flash download aplikas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hari rp ribu khusus buat kamu pilih aktif seka...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>yooo sama sama aku umum kelompok kelas</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>pernah tulis cadar belum pikir warna jeans</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>bu mau kirim</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>berangkat pagi mau tunai transfer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>berapa nomor bri atas nama</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1143 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   teks  label\n",
       "0     promo beli paket flash mulai aplikasi my telko...      1\n",
       "1     gb hari rp ribu spesial buat kamu pilih aktif ...      1\n",
       "2     plg hormat sisa kuota flash download aplikasi ...      1\n",
       "3     mohon hormat sisa kuota flash download aplikas...      1\n",
       "4     hari rp ribu khusus buat kamu pilih aktif seka...      1\n",
       "...                                                 ...    ...\n",
       "1138             yooo sama sama aku umum kelompok kelas      0\n",
       "1139         pernah tulis cadar belum pikir warna jeans      0\n",
       "1140                                       bu mau kirim      0\n",
       "1141                  berangkat pagi mau tunai transfer      0\n",
       "1142                         berapa nomor bri atas nama      0\n",
       "\n",
       "[1143 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('/home/basilmusyaffa19/Skripsi Basil/Dataset/FIX/clean_SMS_22112024.xlsx', engine='openpyxl')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ae28275-17df-4e40-8e23-032aacdd68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_dict = {}\n",
    "    words_set = set()  # membuat set kosong\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]  # kata ada di posisi pertama\n",
    "            vector = np.asarray(values[1:], dtype='float32')  # sisanya adalah vektor embedding\n",
    "            embeddings_dict[word] = vector\n",
    "            words_set.add(word)  # menambahkan kata ke dalam set\n",
    "            \n",
    "    return embeddings_dict, words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f05c2eb1-2c3a-488d-a92e-8c0775879b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove, words_set = load_glove_embeddings('/home/basilmusyaffa19/Skripsi Basil/Embedding Matrix/Dataset Indo/glove_50dim_wiki.id.case.text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14bb9723-ca0c-4b63-82b8-e323b4d572e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 2521\n"
     ]
    }
   ],
   "source": [
    "with open('/home/basilmusyaffa19/Skripsi Basil/Embedding Matrix/Dataset Indo/indo_keras_tokenizer.pickle', 'rb') as handle:\n",
    "    keras_tokenizer = pickle.load(handle)\n",
    "    \n",
    "vocab_size = len(keras_tokenizer.word_index) + 1\n",
    "print('Vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "250311fe-ed0d-4406-85ae-26b2237aa0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data sebelum: 1143\n",
      "Jumlah data setelah: 1138\n"
     ]
    }
   ],
   "source": [
    "print(\"Jumlah data sebelum:\", len(df))\n",
    "\n",
    "# Menghapus data kosong\n",
    "df = df.replace('', np.nan).dropna()\n",
    "# Hapus NaN\n",
    "df = df.dropna(subset=['teks'])\n",
    "# Menghapus nilai float\n",
    "df = df[~df['teks'].apply(lambda x: isinstance(x, float))]\n",
    "# Menghapus semua baris yang duplikat\n",
    "df = df.drop_duplicates(subset=['teks'], keep='first')\n",
    "\n",
    "print(\"Jumlah data setelah:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4a06170-7dd4-402c-850a-80d7b0cd689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(vocab_list, words_set):\n",
    "    num_words_found = 0\n",
    "    oov = []\n",
    "\n",
    "    for word in tqdm(vocab_list):\n",
    "        if word in words_set:\n",
    "            num_words_found += 1\n",
    "        else:\n",
    "            oov.append(word)\n",
    "\n",
    "    total_words = len(vocab_list)\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(num_words_found / total_words))\n",
    "    print('Number of words not found: {}'.format(len(oov)))\n",
    "    \n",
    "    return oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e2d06e6-5a1a-4be8-87eb-fc7169c50491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2520/2520 [00:00<00:00, 551306.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 79.37% of vocab\n",
      "Number of words not found: 520\n",
      "\n",
      "Beberapa kata yang tidak ditemukan:\n",
      "'langgan'\n",
      "'nelpon'\n",
      "'tsel'\n",
      "'silah'\n",
      "'tcash'\n",
      "'simcard'\n",
      "'ooredoo'\n",
      "'yaris'\n",
      "'dakota'\n",
      "'bni'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_list = list(keras_tokenizer.word_index.keys())\n",
    "oov = check_coverage(vocab_list, words_set)\n",
    "\n",
    "print(\"\\nBeberapa kata yang tidak ditemukan:\")\n",
    "for word in oov[:10]:\n",
    "    print(f\"'{word}'\")\n",
    "\n",
    "df_oov = pd.DataFrame(oov, columns=['Kata OOV'])\n",
    "df_oov.to_excel('/home/basilmusyaffa19/Skripsi Basil/Embedding Matrix/kata_oov_indo.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51d35e74-a28c-45c1-b630-d195dd575718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2521, 50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e539e2af-cfe1-4fb4-91c6-374266b43f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_embedding_matrix(tokenizer, oov_words, embedding_matrix, model, embedding_dim=50):\n",
    "    start_time = time.time()\n",
    "    word_embeddings = {}\n",
    "    \n",
    "    for word, i in tqdm(tokenizer.word_index.items(), desc=\"Creating embedding matrix\"): \n",
    "        if word in oov_words:\n",
    "            random_vector = np.random.uniform(-0.25, 0.25, embedding_dim)\n",
    "            random_vector = random_vector / np.linalg.norm(random_vector) * np.sqrt(embedding_dim)\n",
    "            embedding_matrix[i] = random_vector\n",
    "            word_embeddings[i] = (word, random_vector)\n",
    "        else:  \n",
    "            embedding_vector = model[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            word_embeddings[i] = (word, embedding_vector)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nTotal waktu pembuatan embedding matrix: {elapsed_time}s\")\n",
    "    \n",
    "    return embedding_matrix, word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88333845-60c3-406e-b111-ff76e52c1ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embedding matrix: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2520/2520 [00:00<00:00, 91639.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total waktu pembuatan embedding matrix: 0.02921009063720703s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_glove, word_embeddings_glove = initialize_embedding_matrix(\n",
    "    keras_tokenizer,\n",
    "    oov,\n",
    "    embedding_matrix,\n",
    "    glove\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b64d501-1777-45ae-a1c8-98174694e9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.059917  , -1.13609898,  0.55175501, ...,  0.81516302,\n",
       "         2.64219594,  1.23838198],\n",
       "       [-0.51291502, -0.117982  ,  0.019041  , ..., -0.15569399,\n",
       "         0.212929  , -0.38389   ],\n",
       "       ...,\n",
       "       [-0.30222201,  0.47868001,  0.423637  , ..., -0.77613503,\n",
       "         0.97546601,  0.42256501],\n",
       "       [ 0.72796738, -0.61885576,  1.12429665, ...,  1.49172576,\n",
       "        -0.58190233, -1.56130441],\n",
       "       [ 0.45726499, -0.70696402, -0.124477  , ...,  0.52218401,\n",
       "        -0.29743001, -0.57734901]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66427eb7-2630-42cf-897e-ef1cb59385c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 1\n",
      "Word: dapat\n",
      "Vector (first 10 values): [ 0.059917 -1.136099  0.551755 -0.572297  0.11707   0.925134  0.410752\n",
      "  0.32349  -0.679779  1.096869]\n",
      "---\n",
      "Index: 2\n",
      "Word: info\n",
      "Vector (first 10 values): [-0.512915 -0.117982  0.019041 -0.644253  0.023331 -1.118993  0.236992\n",
      " -1.458259 -0.473929  0.402766]\n",
      "---\n",
      "Index: 3\n",
      "Word: rp\n",
      "Vector (first 10 values): [ 0.306585  0.516463  0.768335 -0.13336  -0.433293  0.572058 -0.14886\n",
      " -0.548269 -0.513062 -0.245573]\n",
      "---\n",
      "Index: 4\n",
      "Word: pin\n",
      "Vector (first 10 values): [ 0.026798 -0.651772  0.408602 -0.512624 -0.181467  0.222163  0.619997\n",
      " -1.22604   0.331878  0.049833]\n",
      "---\n",
      "Index: 5\n",
      "Word: sms\n",
      "Vector (first 10 values): [-1.345279 -0.931068  0.340035 -0.479427 -0.135056  0.04676   1.492784\n",
      " -0.725813 -0.659199  0.020117]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for idx, (word, vector) in list(word_embeddings_glove.items())[:5]:\n",
    "    print(f\"Index: {idx}\")\n",
    "    print(f\"Word: {word}\")\n",
    "    vector_np = np.array(vector) if not isinstance(vector, np.ndarray) else vector\n",
    "    print(f\"Vector (first 10 values): {vector_np[:10]}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04b77d88-3aeb-44d3-90b4-981e74bcf8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 66\n",
      "Word: nelpon\n",
      "Vector: [-0.7929115  -0.06483223  1.23008954 -1.12152042  0.08474467  0.43574067\n",
      "  1.52861666 -0.53303626 -1.41711722 -1.23748557  0.38321144  0.98999276\n",
      "  0.84256983 -0.57486092 -1.37248084  1.73947026 -0.18415939  0.51975222\n",
      "  1.61336982 -0.17190405  0.72490277 -0.71027314 -0.38127465 -1.69477436\n",
      "  1.39655164 -1.30033129 -1.51753616 -0.67207073 -1.66574078  1.59974042\n",
      " -0.59118214 -0.43634364  1.75311939  0.7593595   1.35436137 -1.02980878\n",
      " -1.48101734  0.5230628   1.30426909 -0.38585012  0.61321928 -0.19976665\n",
      "  0.48616177  0.43719156  1.16700974 -0.24616884 -0.42664793  0.38214427\n",
      "  0.06694159  0.66921197]\n"
     ]
    }
   ],
   "source": [
    "target_word = \"nelpon\"\n",
    "\n",
    "word_found = False\n",
    "for idx, (word, vector) in word_embeddings_glove.items():\n",
    "    if word == target_word:\n",
    "        print(f\"Index: {idx}\")\n",
    "        print(f\"Word: {word}\")\n",
    "        print(f\"Vector: {vector}\")\n",
    "        word_found = True\n",
    "        break\n",
    "\n",
    "if not word_found:\n",
    "    print(f\"Kata '{target_word}' tidak ditemukan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a8c7f39-dd0e-4b0a-83ab-3250acebf5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/basilmusyaffa19/Skripsi Basil/Embedding Matrix/Dataset Indo/Hasil Embedding/21 Nov/embedding_matrix_gloveWiki_50D_21112024.npy'\n",
    "np.save(path, embedding_matrix_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d5e8da3-a2ff-458e-8fa6-6b298d67b7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukuran file: 0.96 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_size = os.path.getsize('/home/basilmusyaffa19/Skripsi Basil/Embedding Matrix/Dataset Indo/Hasil Embedding/21 Nov/embedding_matrix_gloveWiki_50D_21112024.npy')\n",
    "print(f\"Ukuran file: {file_size/1024/1024:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Skripsi_2",
   "language": "python",
   "name": "skripsi_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
