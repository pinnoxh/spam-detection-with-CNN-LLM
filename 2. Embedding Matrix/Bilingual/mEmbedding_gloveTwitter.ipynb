{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c7093d4-1be6-41a2-92b0-0e186794a66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 12:06:36.851537: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-12 12:06:36.869648: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-12 12:06:36.875081: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-12 12:06:36.889426: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-12 12:06:38.280615: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/basilmusyaffa19/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import pickle\n",
    "nltk.download('punkt')\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f258178-edcd-4a59-8d1e-7ed72b22e1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "1.13.0+cu117\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5aebe2d-8177-4f46-8b2d-6c467171341c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 11.7\n",
      "Number of GPUs: 1\n",
      "GPU 0: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs: {num_gpus}\")\n",
    "\n",
    "if num_gpus > 0:\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2271fd6f-a8f5-4a72-8e0e-d7c5562b1b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ae28275-17df-4e40-8e23-032aacdd68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "glove = GloVe(name='twitter.27B', dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14bb9723-ca0c-4b63-82b8-e323b4d572e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8771\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/home/basilmusyaffa19/Skripsi Basil/Embedding Matrix/Dataset Multilingual/multilingual_keras_tokenizer.pickle', 'rb') as handle:\n",
    "    keras_tokenizer = pickle.load(handle)\n",
    "    \n",
    "vocab_size = len(keras_tokenizer.word_index) + 1\n",
    "print('Vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "476f8748-5e1f-40c1-8fa6-5936a6262b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>teks</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>babe go day sip cappuccino think love send kis...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sy dr rimbawan dirmawa ipb hormat jajang roni ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>send logo ur lover name joined heart txt love ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fine difficulty phone work mine pls send anoth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>already sabarish asked go</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6131</th>\n",
       "      <td>freemsg today day ready horny live town love s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6132</th>\n",
       "      <td>selamat dapat voucher zalora rp ribu masuk kod...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6133</th>\n",
       "      <td>terima kasih hasil laku isi pulsa smartfren ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6134</th>\n",
       "      <td>pangmeulikeun heula kredit bpa simpati ribu ga...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6135</th>\n",
       "      <td>unbelievable faglord</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6136 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   teks  label\n",
       "0     babe go day sip cappuccino think love send kis...      1\n",
       "1     sy dr rimbawan dirmawa ipb hormat jajang roni ...      1\n",
       "2     send logo ur lover name joined heart txt love ...      1\n",
       "3     fine difficulty phone work mine pls send anoth...      1\n",
       "4                             already sabarish asked go      1\n",
       "...                                                 ...    ...\n",
       "6131  freemsg today day ready horny live town love s...      1\n",
       "6132  selamat dapat voucher zalora rp ribu masuk kod...      1\n",
       "6133  terima kasih hasil laku isi pulsa smartfren ma...      1\n",
       "6134  pangmeulikeun heula kredit bpa simpati ribu ga...      1\n",
       "6135                               unbelievable faglord      1\n",
       "\n",
       "[6136 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('/home/basilmusyaffa19/Skripsi Basil/Dataset/FIX/clean_multilingual_22112024.xlsx', engine='openpyxl')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "250311fe-ed0d-4406-85ae-26b2237aa0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data sebelum: 6136\n",
      "Jumlah data setelah: 6136\n"
     ]
    }
   ],
   "source": [
    "print(\"Jumlah data sebelum:\", len(df))\n",
    "\n",
    "# Menghapus data kosong\n",
    "df = df.replace('', np.nan).dropna()\n",
    "# Hapus NaN\n",
    "df = df.dropna(subset=['teks'])\n",
    "# Menghapus nilai float\n",
    "df = df[~df['teks'].apply(lambda x: isinstance(x, float))]\n",
    "# Menghapus semua baris yang duplikat\n",
    "df = df.drop_duplicates(subset=['teks'], keep='first')\n",
    "\n",
    "print(\"Jumlah data setelah:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4a06170-7dd4-402c-850a-80d7b0cd689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(vocab_list, glove):\n",
    "    embedding_words = set(glove.itos)\n",
    "    num_words_found = 0\n",
    "    oov = []\n",
    "\n",
    "    for word in tqdm(vocab_list):\n",
    "        if word in embedding_words:\n",
    "            num_words_found += 1\n",
    "        else:\n",
    "            oov.append(word)\n",
    "\n",
    "    total_words = len(vocab_list)\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(num_words_found / total_words))\n",
    "    print('Number of words not found: {}'.format(len(oov)))\n",
    "    \n",
    "    return oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e2d06e6-5a1a-4be8-87eb-fc7169c50491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8770/8770 [00:00<00:00, 516129.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 88.61% of vocab\n",
      "Number of words not found: 999\n",
      "\n",
      "Beberapa kata yang tidak ditemukan:\n",
      "'tcash'\n",
      "'chikku'\n",
      "'freemsg'\n",
      "'rakhesh'\n",
      "'pobox'\n",
      "'indosatooredoo'\n",
      "'savamob'\n",
      "'bronet'\n",
      "'tronik'\n",
      "'mytelkomsel'\n"
     ]
    }
   ],
   "source": [
    "vocab_list = list(keras_tokenizer.word_index.keys())\n",
    "oov = check_coverage(vocab_list, glove)\n",
    "\n",
    "print(\"\\nBeberapa kata yang tidak ditemukan:\")\n",
    "for word in oov[:10]:\n",
    "    print(f\"'{word}'\")\n",
    "    \n",
    "df_oov = pd.DataFrame(oov, columns=['Kata OOV'])\n",
    "df_oov.to_excel('/home/basilmusyaffa19/Skripsi Basil/Embedding Matrix/kata_oov_multilingual.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51d35e74-a28c-45c1-b630-d195dd575718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8771, 200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 200\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e539e2af-cfe1-4fb4-91c6-374266b43f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_embedding_matrix(tokenizer, oov_words, embedding_matrix, model, embedding_dim=200):\n",
    "    start_time = time.time()\n",
    "    word_embeddings = {}\n",
    "    \n",
    "    for word, i in tqdm(tokenizer.word_index.items(), desc=\"Creating embedding matrix\"): \n",
    "        if word in oov_words:\n",
    "            random_vector = np.random.uniform(-0.25, 0.25, embedding_dim)\n",
    "            random_vector = random_vector / np.linalg.norm(random_vector) * np.sqrt(embedding_dim)\n",
    "            embedding_matrix[i] = random_vector\n",
    "            word_embeddings[i] = (word, random_vector)\n",
    "        else:  \n",
    "            embedding_vector = model[word].numpy()\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            word_embeddings[i] = (word, embedding_vector)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nTotal waktu pembuatan embedding matrix: {elapsed_time}s\")\n",
    "    \n",
    "    return embedding_matrix, word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88333845-60c3-406e-b111-ff76e52c1ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embedding matrix: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8770/8770 [00:00<00:00, 16296.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total waktu pembuatan embedding matrix: 0.547499418258667s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_glove, word_embeddings_glove = initialize_embedding_matrix(\n",
    "    keras_tokenizer,\n",
    "    oov,\n",
    "    embedding_matrix,\n",
    "    glove\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b64d501-1777-45ae-a1c8-98174694e9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.36776   ,  0.62058002, -0.16424   , ..., -0.31200001,\n",
       "        -0.02699   ,  0.084279  ],\n",
       "       [-0.20837   ,  0.37200999, -0.066035  , ..., -0.0091251 ,\n",
       "        -0.26668999,  0.29602   ],\n",
       "       ...,\n",
       "       [-0.075862  ,  0.1478    , -0.18271001, ..., -0.16144   ,\n",
       "         0.12695999, -0.67714   ],\n",
       "       [-0.24065415, -1.64383852, -1.16437044, ..., -1.33313845,\n",
       "        -1.59457131,  1.47274856],\n",
       "       [-1.32298282,  0.1102973 , -0.06383698, ..., -0.63275833,\n",
       "         0.46027717,  0.77345273]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66427eb7-2630-42cf-897e-ef1cb59385c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 1\n",
      "Word: call\n",
      "Vector (first 10 values): [-0.36776   0.62058  -0.16424  -0.55841   0.19918  -0.021146  0.70505\n",
      "  0.27204   0.53151  -0.3686  ]\n",
      "---\n",
      "Index: 2\n",
      "Word: get\n",
      "Vector (first 10 values): [-0.20837   0.37201  -0.066035 -0.44603   0.37284  -0.12545   0.86787\n",
      "  0.4099    0.23244   0.3407  ]\n",
      "---\n",
      "Index: 3\n",
      "Word: ur\n",
      "Vector (first 10 values): [ 0.21273   0.11611   0.16871  -0.35413   0.13989  -0.37088   0.78829\n",
      "  0.008818 -0.048655  0.65067 ]\n",
      "---\n",
      "Index: 4\n",
      "Word: go\n",
      "Vector (first 10 values): [ 0.24313   0.011597  0.38135  -0.5835    0.14441   0.56247   0.52358\n",
      "  0.20859   0.3643    0.16414 ]\n",
      "---\n",
      "Index: 5\n",
      "Word: ok\n",
      "Vector (first 10 values): [-0.039412 -0.39475  -0.34866   0.14839  -0.56289   0.20934   0.35344\n",
      "  0.16257   0.027069  0.27543 ]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for idx, (word, vector) in list(word_embeddings_glove.items())[:5]:\n",
    "    print(f\"Index: {idx}\")\n",
    "    print(f\"Word: {word}\")\n",
    "    vector_np = np.array(vector) if not isinstance(vector, np.ndarray) else vector\n",
    "    print(f\"Vector (first 10 values): {vector_np[:10]}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04b77d88-3aeb-44d3-90b4-981e74bcf8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 531\n",
      "Word: chikku\n",
      "Vector: [-0.88899276 -0.73291837  0.60996541 -0.79552452  0.01565385  0.86605629\n",
      "  0.601596   -0.72080997 -1.1569533  -1.02313873 -0.01146402  0.30010013\n",
      " -0.36260153 -1.17950303  0.12731209 -0.27679852 -1.53906447 -0.68430769\n",
      " -0.92459931 -0.79644716 -1.2356532  -1.37369282  0.65013007 -1.26218849\n",
      "  1.68260395  0.25148757  0.44901461 -0.07733233  1.35351462  1.37775557\n",
      " -0.18565755  0.73124169  1.71877217  1.31947088  1.26259106  1.71532353\n",
      " -0.5110958  -0.19901703  0.43186352  1.42772439  0.58080063 -1.45906192\n",
      "  0.18456078 -0.71246544  0.26679146  0.77731645  0.49631838 -0.28769978\n",
      " -0.6189252   1.64811938 -0.20862224 -0.55636232 -1.62189047  0.13780929\n",
      "  1.52216887  0.59552742  1.59737255  1.25341108  0.36921556  1.16894229\n",
      " -0.02247133 -1.2947174   0.91390108 -1.27624297 -0.2760889   0.46732463\n",
      " -1.56917048  0.32893229 -0.92997065  0.8475611  -0.48986531  0.36904913\n",
      "  0.44480049 -0.00896775 -0.36865587 -0.62337652  0.94076459  1.24863189\n",
      " -1.70111327 -0.94019568 -0.21936539 -0.56675523 -0.1403914  -0.82695046\n",
      "  0.7591356  -0.95314436 -0.12805306 -1.66109557  0.92457002 -0.23754902\n",
      " -1.27838647  0.0047665  -1.19877698  0.68905391 -1.39290298 -0.7483672\n",
      " -1.15036002 -0.72978528 -1.62924023  0.2980989  -0.09243954 -1.49189947\n",
      "  1.09099183 -0.07963639  1.02670583 -0.4795393   1.4737758   1.43786663\n",
      "  1.43067733  0.51636898 -1.19743167 -0.73130906 -0.26327896 -0.22029484\n",
      " -1.15173534 -1.25479556 -1.52543703  0.68793986 -1.72764314 -1.27684986\n",
      " -0.59660353  0.71112589  0.30700981 -0.52518508 -1.66531553 -1.45174034\n",
      " -0.86464944 -0.38037497  1.53294442  0.42319569  0.34289152  1.52689341\n",
      " -0.87538161  0.76774782  1.30556484 -0.78039222  1.45368281  1.25521325\n",
      " -1.71672866 -0.02915843 -0.01552075  0.52114223  1.49733245  0.678888\n",
      "  1.65899043 -1.62504533 -0.00698867  0.2811615   1.72334497  0.30708178\n",
      "  0.82492553  0.41892753 -0.94171573  0.74118398 -1.41432556 -1.02887833\n",
      "  1.28317915 -1.54599589  1.29795863 -0.82839751 -0.12362517  0.08157107\n",
      " -0.49176184 -1.38113919  0.83575769  0.16042781 -0.63939941 -1.20973425\n",
      " -1.39779982  0.17736887  1.34556427  0.87344373  1.68503966  1.35378902\n",
      "  0.39798946  1.02700471  1.01262351 -0.27934069 -0.41734319  0.184591\n",
      "  1.7033904   0.99395121  0.63229064 -1.32925421  0.1447719  -1.63337021\n",
      "  1.37203142 -0.7513436  -0.94498205  1.11416144 -0.96523219 -0.73511587\n",
      " -1.63139847 -1.07177969 -1.32590837 -1.50208924 -0.06660322  0.70721042\n",
      " -1.7142365   0.00676072]\n"
     ]
    }
   ],
   "source": [
    "target_word = \"chikku\"\n",
    "\n",
    "word_found = False\n",
    "for idx, (word, vector) in word_embeddings_glove.items():\n",
    "    if word == target_word:\n",
    "        print(f\"Index: {idx}\")\n",
    "        print(f\"Word: {word}\")\n",
    "        print(f\"Vector: {vector}\")\n",
    "        word_found = True\n",
    "        break\n",
    "\n",
    "if not word_found:\n",
    "    print(f\"Kata '{target_word}' tidak ditemukan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a8c7f39-dd0e-4b0a-83ab-3250acebf5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/basilmusyaffa19/Skripsi Basil/Embedding Matrix/Dataset Multilingual/Hasil Embedding/21 Nov/embedding_matrix_gloveTwitter_200D_21112024.npy'\n",
    "np.save(path, embedding_matrix_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d5e8da3-a2ff-458e-8fa6-6b298d67b7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukuran file: 13.38 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_size = os.path.getsize('/home/basilmusyaffa19/Skripsi Basil/Embedding Matrix/Dataset Multilingual/Hasil Embedding/21 Nov/embedding_matrix_gloveTwitter_200D_21112024.npy')\n",
    "print(f\"Ukuran file: {file_size/1024/1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58aea3-862d-42b0-8144-082004d1b8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Skripsi_2",
   "language": "python",
   "name": "skripsi_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
